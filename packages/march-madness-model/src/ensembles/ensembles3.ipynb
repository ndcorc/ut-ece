{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import VotingClassifier as VC\n",
    "from sklearn.ensemble import ExtraTreesClassifier as ETC\n",
    "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "#iris = datasets.load_iris()\n",
    "#X, y = iris.data[:, 1:3], iris.target\n",
    "df = pd.read_csv(\"Matchup_KP.csv\")\n",
    "features = ['FieldGoalAvg', 'ThreePointAvg', 'FreeThrowAvg', 'ReboundAvg', 'AssistAvg', 'TurnOverAvg', \n",
    "            'StealAvg', 'BlockAvg', 'PersonalFoulAvg', 'Pyth', 'Rank', 'AdjustO', 'AdjustO Rank', 'AdjustD', \n",
    "            'AdjustD Rank', 'AdjustT', 'AdjustT Rank', 'Luck', 'Luck Rank']\n",
    "features2 = ['Pyth', 'Rank', 'AdjustO', 'AdjustO Rank', 'AdjustD', \n",
    "            'AdjustD Rank', 'AdjustT', 'AdjustT Rank', 'Luck', 'Luck Rank']\n",
    "# Load Elo Ratings (dict)\n",
    "team_elos = pickle.load(open(\"../elo/data/team_elos.p\", \"rb\"))\n",
    "# Extremely Random Forest Features\n",
    "features3 = ['Pyth', 'AdjustO', 'AdjustD', 'Luck','FieldGoalAvg']\n",
    "# Random Forest Features\n",
    "features4 = ['Pyth', 'AdjustO', 'AdjustD', 'Luck','BlockAvg']\n",
    "# Gradient Boosting Features\n",
    "features5 = ['Pyth', 'AdjustO', 'Luck','BlockAvg','AssistAvg']\n",
    "#ADA Boost\n",
    "features6 = ['Pyth','Luck','AdjustO','AdjustD','FieldGoalAvg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Submission = pd.read_csv(\"2016Submission.csv\")\n",
    "submission_test = Submission[features2]\n",
    "\n",
    "X = df[features2]\n",
    "y = df.WinLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_best_model(model, parameters):\n",
    "    m = model()\n",
    "    m = GridSearchCV(m, parameters)\n",
    "    m.fit(X, y)\n",
    "    return m.best_estimator_, m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Loss', 'Win'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Loss', 'Win'])\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def accuracies(cm):\n",
    "    totals = np.sum(cm, 1)\n",
    "    no_acc = cm[0, 0] / totals[0]\n",
    "    yes_acc = cm[1, 1] / totals[1]\n",
    "    print(\"Loss Acc: {0}\".format(no_acc))\n",
    "    print(\"Win Acc: {0}\".format(yes_acc))\n",
    "    print(\"Avg Acc: {0}\".format((no_acc + yes_acc)/2))\n",
    "    \n",
    "\n",
    "def run_model(model):\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plot_confusion_matrix(cm)\n",
    "    accuracies(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_elo(season, team):\n",
    "    try:\n",
    "        return team_elos[season][team]\n",
    "    except:\n",
    "        try:\n",
    "            # Get the previous season's ending value.\n",
    "            team_elos[season][team] = team_elos[season-1][team]\n",
    "            return team_elos[season][team]\n",
    "        except:\n",
    "            # Get the starter elo.\n",
    "            team_elos[season][team] = base_elo\n",
    "            return team_elos[season][team]\n",
    "        \n",
    "def get_elo_diff(team_1, team_2, season):\n",
    "    elo1 = get_elo(season, team_1)\n",
    "    elo2 = get_elo(season, team_2)\n",
    "    return elo1 - elo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_elos():\n",
    "    dFrames = [df, Submission]\n",
    "    for i, dFrame in enumerate(dFrames):\n",
    "        elo_diffs = []\n",
    "        for index, row in dFrame.iterrows():\n",
    "            elo_diffs.append(get_elo_diff(row['Wteam'], row['Lteam'], row['Year']))\n",
    "        elos = pd.DataFrame(elo_diffs, columns = ['Elo Rank'])\n",
    "        if i==0:\n",
    "            X_elos = pd.concat([elos, X], axis=1)\n",
    "        else:\n",
    "            submission_elos = pd.concat([elos, submission_test], axis=1)\n",
    "    return X_elos, submission_elos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for %s.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, n_jobs=-1)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs.best_estimator_\n",
    "            \n",
    "    def score_summary(this, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "\n",
    "        rows = [row(k, gsc.cv_validation_scores, gsc.parameters) \n",
    "                for k in this.keys\n",
    "                for gsc in this.grid_searches[k].grid_scores_]\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = { \n",
    "    'ExtraTreesClassifier': ETC(),\n",
    "    'RandomForestClassifier': RFC(),\n",
    "    'AdaBoostClassifier': ABC(),\n",
    "    'GradientBoostingClassifier': GBC(),\n",
    "#    'SVC': SVC(),\n",
    "    'LogisticRegression': LR()\n",
    "}\n",
    "\n",
    "params = { \n",
    "    'ExtraTreesClassifier': {'n_estimators':[10, 16, 32, 50, 100], 'max_features': ['sqrt']},\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators':[10, 16, 32, 50, 100, 300, 500, 1000],\n",
    "        'max_features': [ None, 'auto', 'sqrt', 'log2']\n",
    "    },\n",
    "    'AdaBoostClassifier':  { 'n_estimators': [10, 16, 32, 50, 100, 500, 1000] },\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators':[10, 16, 32, 50, 100, 500, 1000], \n",
    "        'learning_rate':[0.01, 0.8, 1]\n",
    "    },\n",
    "#    'SVC': [\n",
    "#        {'kernel': ['linear'], 'C': [1, 10]},\n",
    "#        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n",
    "#    ],\n",
    "    'LogisticRegression': { 'penalty': ['l1', 'l2'], 'C': np.logspace(-4, 4, 8) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Running GridSearchCV for ExtraTreesClassifier.\n",
      "Running GridSearchCV for RandomForestClassifier.\n"
     ]
    }
   ],
   "source": [
    "helper = EstimatorSelectionHelper(models, params)\n",
    "helper.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for LogisticRegression.\n",
      "Running GridSearchCV for AdaBoostClassifier.\n",
      "Running GridSearchCV for GradientBoostingClassifier.\n",
      "Running GridSearchCV for ExtraTreesClassifier.\n",
      "Running GridSearchCV for RandomForestClassifier.\n"
     ]
    }
   ],
   "source": [
    "X_elos, submission_elos = append_elos()\n",
    "cols = X_elos.columns.tolist()\n",
    "cols = cols[0:7] + cols[7:][::-1]\n",
    "X_elos = X_elos[cols]\n",
    "submission_elos = submission_elos[cols]\n",
    "del X_elos['AdjustT']\n",
    "del submission_elos['AdjustT']\n",
    "helper2 = EstimatorSelectionHelper(models, params)\n",
    "helper2.fit(X_elos, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in helper2.grid_searches.keys():\n",
    "    clf = helper2.grid_searches[model]\n",
    "    predictions = clf.predict_proba(submission_elos)\n",
    "    Submission.Pred = predictions[:,1]\n",
    "    model_DF = Submission[['Id','Pred']].sort_values(by = 'Id')\n",
    "    model_DF.to_csv(\"results3/\"+model+\"submissionELO.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24958628  0.14385444  0.12218565  0.0830409   0.07379973  0.06188813\n",
      "  0.0690053   0.06429626  0.063806    0.06853731]\n",
      "\n",
      "\n",
      "[ 0.14867886  0.14081998  0.13522297  0.10397777  0.10023914  0.08291633\n",
      "  0.09042629  0.06762896  0.06441525  0.06567447]\n",
      "\n",
      "\n",
      "[  9.09965064e-01   4.77051131e-02   0.00000000e+00   1.99648178e-02\n",
      "   9.18177752e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   4.72051876e-04   1.27111753e-02]\n"
     ]
    }
   ],
   "source": [
    "print helper2.grid_searches['RandomForestClassifier'].feature_importances_\n",
    "print \"\\n\"\n",
    "print helper2.grid_searches['ExtraTreesClassifier'].feature_importances_\n",
    "print \"\\n\"\n",
    "print helper2.grid_searches['GradientBoostingClassifier'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in helper.grid_searches.keys():\n",
    "    clf = helper.grid_searches[model]\n",
    "    predictions = clf.predict_proba(submission_test)\n",
    "    Submission.Pred = predictions[:,1]\n",
    "    model_DF = Submission[['Id','Pred']].sort_values(by = 'Id')\n",
    "    model_DF.to_csv(\"results3/\"+model+\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estims = [\n",
    "    ('lr', helper.grid_searches['LogisticRegression']),\n",
    "    ('rfc', helper.grid_searches['RandomForestClassifier']),\n",
    "    ('etc', helper.grid_searches['ExtraTreesClassifier']),\n",
    "    ('gbc', helper.grid_searches['GradientBoostingClassifier'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voting': 'soft', 'weights': [1, 4, 7, 0.8]}\n",
      "{'voting': 'soft', 'weights': [1, 5, 6, 0.8]}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'voting':['soft'],\n",
    "    'weights': [\n",
    "        [1, 10, 10, 0.8],\n",
    "        [1, 5, 5, 0.8],\n",
    "        [1, 6, 5, 0.8],\n",
    "        [1, 5, 6, 0.8], \n",
    "        [1, 7, 4, 0.8],\n",
    "        [1, 4, 7, 0.8],\n",
    "        [1, 8, 3, 0.8], \n",
    "        [1, 3, 8, 0.8]\n",
    "    ]\n",
    "}\n",
    "vc = VC(estimators=estims)\n",
    "grid1 = GridSearchCV(vc, params, n_jobs=-1)\n",
    "grid2 = GridSearchCV(vc, params, n_jobs=-1)\n",
    "grid1.fit(X, y)\n",
    "print grid1.best_params_\n",
    "grid2.fit(X_elos, y)\n",
    "print grid2.best_params_\n",
    "vc1 = grid1.best_estimator_\n",
    "vc2 = grid2.best_estimator_\n",
    "preds1 = vc1.predict_proba(submission_test)\n",
    "preds2 = vc2.predict_proba(submission_elos)\n",
    "\n",
    "Submission.Pred = preds1[:,1]\n",
    "EnsembleSubmission1 = Submission[['Id','Pred']].sort_values(by = 'Id')\n",
    "EnsembleSubmission1.to_csv(\"results3/votingSubmission.csv\",index = False)\n",
    "\n",
    "Submission.Pred = preds2[:,1]\n",
    "EnsembleSubmission2 = Submission[['Id','Pred']].sort_values(by = 'Id')\n",
    "EnsembleSubmission2.to_csv(\"results3/votingSubmissionELO.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
