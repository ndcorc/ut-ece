{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-361M Introduction to Data Mining\n",
    "## Assignment #3\n",
    "## Due: Thursday, Mar 4, 2016 by midnight; Total points: 50\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook** (if this isn't possible, let me know). Please use this naming format for your notebook you submit: **Group(Group Num)_HW(HW Number).ipynb**. For example, Group1_HW1.ipynb. Homeworks should be submitted through Canvas in your **groups of 3 from the first homework**. If groups need to be adjusted please contact the TA. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1 (2+1 = 3 points)\n",
    "\n",
    "View the video at:\n",
    "\n",
    "https://www.youtube.com/watch?v=jbkSRLYSojo\n",
    "\n",
    "(Hans Rosling's 200 Countries, 200 Years, 4 Minutes) and answer the following questions:\n",
    "\n",
    "1. How many variables are being visualized in the “moving bubble plots” video (list them)?\n",
    "\n",
    "2. Identify a variable that is “zoomed into”, i.e., examined at a sub-category or more detailed level.\n",
    "\n",
    "\n",
    "FACTOID: Rosling’s gapminder visualization\n",
    "\n",
    "(see https://www.youtube.com/user/Gapcast for some more insightful videos) can now be\n",
    "\n",
    "readily used by you via Google Charts: https://developers.google.com/chart/interactive/docs/gallery\n",
    "\n",
    "Just plug in your own variables into “Bubble Chart” under the URL above and go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 2 (3+3+2+2=10 points)\n",
    "\n",
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The dataset is prostate cancer data. The data can be found on canvas on the homework 3 page as prostate.csv. More information on the data can be found [here](https://cran.r-project.org/web/packages/ElemStatLearn/ElemStatLearn.pdf) under prostate. Use a random state of 42 and a test size of 1/3 to [split the data into training and test](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html). We will be using all the variables to predict lcavol. \n",
    "\n",
    "1. Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter lambda (Note: lambda is called alpha in sklearn), use [0.00001, 0.0001,0.001, 0.005, 0.01, 0.05, 0.1, 1, 5, 10, 100]  for ridge and [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5] for lasso. Report the best chosen based on cross-validation. The cross validation should happen on your training data using  average MSE as the scoring metric.\n",
    "2. Run ridge and lasso for all of the parameters specified above (on all training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? \n",
    "3. Run least squares regression, ridge, and lasso on the full training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error on the test data for each.\n",
    "4. For the best lasso parameter, determine the variables that were not dropped. Using only these variables, run least squares regression on full training data and report the prediction error on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "prostate_data = pd.read_csv(\"prostate.csv\", sep= ',')\n",
    "#prostate_data.head()\n",
    "Y = prostate_data.lcavol.values\n",
    "x = prostate_data.copy()\n",
    "del x['lcavol']\n",
    "X = x.values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 42)\n",
    "alphas_lasso = np.array([0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "alphas_ridge = np.array([0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 5, 10, 100])\n",
    "alpha_errors = list()\n",
    "lasso_MSE_list = list()\n",
    "ridge_MSE_list = list()\n",
    "print(X_train.shape)\n",
    "#lasso_model = linear_model.LassoCV(alphas = alphas_lasso)\n",
    "#ridge_model = linear_model.Ridge(alphas = alphas_ridge)\n",
    "\n",
    "kf = KFold(len(X_train), n_folds = 5)\n",
    "\n",
    "#print(alphas_lasso[i])\n",
    "#print(\"alpha:\", alphas_lasso[i])\n",
    "#print(\"MSE:\", MSE_list[i])\n",
    "\n",
    "for i in range(len(alphas_lasso)):\n",
    "    for k, (train, test) in enumerate(kf):\n",
    "        lasso_model = linear_model.Lasso(alpha = alphas_lasso[i])\n",
    "        lasso_model.fit(X_train[train], Y_train[train])\n",
    "        #print(\"the MSE for this K fold is:\", metrics.mean_squared_error(Y_train[test], lasso_model.predict(X_train[test])))\n",
    "        alpha_errors.append(metrics.mean_squared_error(Y_train[test], lasso_model.predict(X_train[test])))\n",
    "    lasso_MSE_list.append(np.mean(alpha_errors))\n",
    "    del alpha_errors[:]\n",
    "    \n",
    "    \n",
    "for i in range(len(alphas_ridge)):\n",
    "    for k, (train, test) in enumerate(kf):\n",
    "        ridge_model = linear_model.Ridge(alpha = alphas_ridge[i])\n",
    "        ridge_model.fit(X_train[train], Y_train[train])\n",
    "        alpha_errors.append(metrics.mean_squared_error(Y_train[test], ridge_model.predict(X_train[test])))\n",
    "    ridge_MSE_list.append(np.mean(alpha_errors))\n",
    "    del alpha_errors[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (3+3+2+2 = 10 points)\n",
    "\n",
    "Re-solve all the questions in question 2 using R. You can submit the code and results via a PDF or other format. Just please make a reference to it in your notebook. See hints.R on the Canvas homework 3 page to help get you started. I would recommend using [RStudio](https://www.rstudio.com/products/rstudio/download/) for your work in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attached as a PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5+5 = 10 points)\n",
    "\n",
    "1. Derive the coefficent updates, from first principles, for a gradient descent version of linear regression. Hint: start from the cost function. If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for a gradient descent version of linear regression. Should be similar to sklearn in that you have a fit function that takes an X, y, learning rate, and number of iterations and a predict funtion that takes an X value. Use your new SGD regression to re-run question 2.4 and compare MSE. Make sure you always normalize your X matrices and use an intercept. You can also compare your results with SGDRegressor from sklearn, but not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### 4.2\n",
    "learning rate = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "Iteration 0 | Cost: 5195.650702\n",
      "Iteration 1 | Cost: 1193967223.814615\n",
      "Iteration 2 | Cost: 278783674634501.125000\n",
      "Iteration 3 | Cost: 65104024581001158656.000000\n",
      "Iteration 4 | Cost: 15203688707124481493041152.000000\n",
      "Iteration 5 | Cost: 3550504827546945815504316530688.000000\n",
      "Iteration 6 | Cost: 829146450903637814926588314132676608.000000\n",
      "Iteration 7 | Cost: 193629883759936215234480540203482070646784.000000\n",
      "Iteration 8 | Cost: 45218226338695603326125925721043900674722496512.000000\n",
      "Iteration 9 | Cost: 10559774935115513248866860184460888839023251729416192.000000\n",
      "Iteration 10 | Cost: 2466015492183734017879320355603953258605503964631273570304.000000\n",
      "Iteration 11 | Cost: 575886554879841758007930101276977824090924462065906843415216128.000000\n",
      "Iteration 12 | Cost: 134486310058697554318981505171494596035921488584334943257210413645824.000000\n",
      "Iteration 13 | Cost: 31406476570681320008760672307268021202815571650337590367436441808518774784.000000\n",
      "Iteration 14 | Cost: 7334328454355299471018989873538948194898959482855611560750129303685630307336192.000000\n",
      "Iteration 15 | Cost: 1712779647704329251455052357222677799920181517940523781612538426781311717403260878848.000000\n",
      "Iteration 16 | Cost: 399984012148803061895637848911385331155329398440769960786108809387068658162793631805603840.000000\n",
      "Iteration 17 | Cost: 93407934983982236387965575898405689277094100621357813730639852495901315833389909030300031123456.000000\n",
      "Iteration 18 | Cost: 21813477671517353401007582886911788880371956383833668200927326217058965615254188357533439309333397504.000000\n",
      "Iteration 19 | Cost: 5094083368906317231300265021068861884679510587638901593278058558577950836648463611570301332212394500292608.000000\n",
      "Iteration 20 | Cost: 1189617068866161215845292986389236892289740245966864561917788932872232964529632906054422531858230688660771045376.000000\n",
      "Iteration 21 | Cost: 277810288535099741747188726611484770780055618760191034312293656392861747113958862995571183101242076990413792289488896.000000\n",
      "Iteration 22 | Cost: 64876806525241973588307495025533676653832649381427226015033375480163624336565067254144292991817314686868021483994393608192.000000\n",
      "Iteration 23 | Cost: 15150626879615709171142046570192558087240356970816308495225495812196779494309141686463327030623958615725760159253915040512737280.000000\n",
      "Iteration 24 | Cost: 3538113343418425893301711533859662801345698536876896495137371630010879199534521543343968769016509010892217874490853569474815384879104.000000\n",
      "Iteration 25 | Cost: 826252677882133207054430162085043194965676829926754428639262394268551744757352664463656468774718784372079650395900800913714556594521374720.000000\n",
      "Iteration 26 | Cost: 192954103343619024538531766530778267665972815118819541319444126813854413708133002215118171859373776296112555831100123344913285149210414566866944.000000\n",
      "Iteration 27 | Cost: 45060411897934090036077654723847368279246271601469291148547485623215841649564370481559101185641855812708840454881184129263224665840128156575566135296.000000\n",
      "Iteration 28 | Cost: 10522920659508363877304240603173939336126367904391876799783154325396699282984241853494499312080676003778643357659135505997239646374584423236888904681390080.000000\n",
      "Iteration 29 | Cost: 2457408943733706623800660859915523526649815318755182606327097289515008630130793396856510233413874571605963586757799071455191592822279106189118866491701047853056.000000\n",
      "Iteration 30 | Cost: 573876674750538850708691455262911831643773909339406324771543018307101434796554538066860571960161895183886159882842700052516808841863983430508608007553424380676538368.000000\n",
      "Iteration 31 | Cost: 134016944417218648523074875693903039108588389731943500365537115631956395227365208576127053654888330326139274382273228327873063749016339269331724110551610005949573035458560.000000\n",
      "Iteration 32 | Cost: 31296866001280190552274455578241146553947876044003595332285026971233977821178748320681952629412463121439467353011703133028771802466207818309142318082041309256778171770919190528.000000\n",
      "Iteration 33 | Cost: 7308731188891675367209703069129997159794246358990566447726956633418842123256002713273905164159719189490548698767163061117643006113810941347464695685929848634841634450809137341136896.000000\n",
      "Iteration 34 | Cost: 1706801939507070621745190488946674581286632444155183365458960456798767635494790903719996532447116561978494426534983190187043340534094750345190594420510207715462475256643649921154681929728.000000\n",
      "Iteration 35 | Cost: 398588042905825174044459391679226428447358835936595133718813502374469307891453885376950606346019350866138250634444060451930973845344809048187338487009194927868718594010041971469148904207941632.000000\n",
      "Iteration 36 | Cost: 93081935443182542169022169361004833805292318636613832636818071245825777368684261186138276506115628118164377872693404169225348547478556822328245031203306809831586764021676322176497566489584529833984.000000\n",
      "Iteration 37 | Cost: 21737347268833933797040059818631553729307701496960640774583170070665445356017104756038189310014918883472101438381756540434733159510566275428234064514647068199486092154394723849844894615157428514965159936.000000\n",
      "Iteration 38 | Cost: 5076304699039109313433807944396688052664889454830756763722353500734523056365426745848402086711226849298234243939374089828564537903004270460683948267488379594017966366003055063841115612182951871823168774602752.000000\n",
      "Iteration 39 | Cost: 1185465230821095622464184873529895556969587981495010760709061731480009443844240827696754891971193622144323470666833334357926617314261088221182918535422681707697373871249233921521445403551215998393955924153015992320.000000\n",
      "Iteration 40 | Cost: 276840713236092241342629405884317261896598191497127313276704369218728735841207494257726494682200244796179561228317674767448362023649542872208292706367529155416775140107166741231660344536378790824387166217853098969268224.000000\n",
      "Iteration 41 | Cost: 64650382408924795186193475452516714103469540601655812248025123480710858727673093170227144910190875379768214213424824418540257189953152177125280991816855948717224864570711930493303062100833616555959335287612079950540138610688.000000\n",
      "Iteration 42 | Cost: 15097750243316838018473414560940195724227612415927243197330615564044400289980747430960491891737780353908747115317208166324134676125329505740670188325313068912283321624371310119168787796470290502902538267437206554599953941284257792.000000\n",
      "Iteration 43 | Cost: 3525765106350038383657677106805502227766317421735477798217978789791008623776403599837127377120002283033289117747720912765758081852238555826337834872099232993026714841374266417128232699093946183201986920095895209154935525595018350821376.000000\n",
      "Iteration 44 | Cost: 823369004309645956283079980594158110141459389716375556858887047455635761527604162127463172437457557000428802816868695263853273347629183384280277432187422594814115169197498066133457172041623266442865388239244242333509250350937967575559045120.000000\n",
      "Iteration 45 | Cost: 192280681443261286957496826577407440406751021771573873229068910868534082561165554658137077048616330893554344848472890643679394739554113798977231265184060331275076868546110511530761556579629634929997868774901796555723653298979766708460717349011456.000000\n",
      "Iteration 46 | Cost: 44903148239450694938912033792475577612440703243490853212509946005689904101805761026385603951130991700768000930150568574355981322932901980120589485012785528421443357519849303456680495284922583393766927580188390253251647969371163534280641043928805539840.000000\n",
      "Iteration 47 | Cost: 10486195007630325045514406704472815934300493786405385963071945809895304586777241094316531255943927910483297933098529049358018380458590121295028519626949043100958004326169875294506551628508573628091858820835070250480115594721076049644499322286454184211382272.000000\n",
      "Iteration 48 | Cost: 2448832432676579743144029668568501848098508709576745611722669465588325916246049395718294285540360136858226325324075837584983313475034603512943647471943436508341981117324294330374448298964879245171344964588806328524093556341303456141732070712046436609462150103040.000000\n",
      "Iteration 49 | Cost: 571873809228715729470729153225380715929762725632748867038135832015872977000591583371245521716946332274309912004657287458874247209110271543463414030663043721867026045240135628584455555553433640656418268523207075514243864834139902528037407125948401061277669206869934080.000000\n",
      "Iteration 50 | Cost: 133549216891213140700931449792181456170665166660778118824956732000258855433212267285238827325390817262925550580700441712146552781251231254454149946418844232047094534862474643555028776602939661182689045054142804578486636661865009597773562050990464487264807314697495647354880.000000\n",
      "Iteration 51 | Cost: 31187637979628417243635663721880113612153212253838486780520544991581744682323764717340962241549786787996990499550795797621736216923176164805597756062666507558427581897000733948326139055299582270226060963854259421318574901315548280315427355750379083870970081677145387089656807424.000000\n",
      "Iteration 52 | Cost: 7283223259487025840490082536344941554122995868889013729404419525069432771262127210011441341550036422307217418010946673022020774942277003537183929600454533125694513989746054554889033934661857302137897058899446311116670820920514931426895118126920788842078927592994815539869210539720704.000000\n",
      "Iteration 53 | Cost: 1700845093885653875168781032623342547044544033346526909762129446023367050405621316808700925213512009371813463044709790223888956535739705397553365711456713957375319048220144780952222377063736004701555820833698376374059175031446882547544814395264994330651094900848700713582977391133301145600.000000\n",
      "Iteration 54 | Cost: 397196945682899246708018645872930817339003371182737798981236359307334954873963912417774697278603871379180942405830977549765758258795317707153957185785166532902271374110428295347371255262359153474341183534698446930738123784489902037827001636759267601543866914197245377710453705078997014145925120.000000\n",
      "Iteration 55 | Cost: 92757073661189314787795264025781163020184363935549925179420362972483844341757004788810036611221571913724622429991811678189385541461768409954120600349375476390250948662853623285222856512851170447863020438014597257507808929763284818140280918274219761959141531342706269924992037624520796607483272495104.000000\n",
      "Iteration 56 | Cost: 21661482566021974881762687768452089881981537317384609419526525959190925707753979358141493893820316981473932646285280598965338929069447782128360222863162679169937258828436778162951189956200943376111274708056965331996305155317481931750845243928352201116414521269060896259502288493806247093384322240869302272.000000\n",
      "Iteration 57 | Cost: inf\n",
      "Iteration 58 | Cost: inf\n",
      "Iteration 59 | Cost: inf\n",
      "Iteration 60 | Cost: inf\n",
      "Iteration 61 | Cost: inf\n",
      "Iteration 62 | Cost: inf\n",
      "Iteration 63 | Cost: inf\n",
      "Iteration 64 | Cost: inf\n",
      "Iteration 65 | Cost: inf\n",
      "Iteration 66 | Cost: inf\n",
      "Iteration 67 | Cost: inf\n",
      "Iteration 68 | Cost: inf\n",
      "Iteration 69 | Cost: inf\n",
      "Iteration 70 | Cost: inf\n",
      "Iteration 71 | Cost: inf\n",
      "Iteration 72 | Cost: inf\n",
      "Iteration 73 | Cost: inf\n",
      "Iteration 74 | Cost: inf\n",
      "Iteration 75 | Cost: inf\n",
      "Iteration 76 | Cost: inf\n",
      "Iteration 77 | Cost: inf\n",
      "Iteration 78 | Cost: inf\n",
      "Iteration 79 | Cost: inf\n",
      "Iteration 80 | Cost: inf\n",
      "Iteration 81 | Cost: inf\n",
      "Iteration 82 | Cost: inf\n",
      "Iteration 83 | Cost: inf\n",
      "Iteration 84 | Cost: inf\n",
      "Iteration 85 | Cost: inf\n",
      "Iteration 86 | Cost: inf\n",
      "Iteration 87 | Cost: inf\n",
      "Iteration 88 | Cost: inf\n",
      "Iteration 89 | Cost: inf\n",
      "Iteration 90 | Cost: inf\n",
      "Iteration 91 | Cost: inf\n",
      "Iteration 92 | Cost: inf\n",
      "Iteration 93 | Cost: inf\n",
      "Iteration 94 | Cost: inf\n",
      "Iteration 95 | Cost: inf\n",
      "Iteration 96 | Cost: inf\n",
      "Iteration 97 | Cost: inf\n",
      "Iteration 98 | Cost: inf\n",
      "Iteration 99 | Cost: inf\n",
      "[ nan  nan  nan  nan  nan  nan  nan  nan]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# m denotes the number of examples here, not the number of features\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        loss = hypothesis - y\n",
    "        # avg cost per example (the 2 in 2*m doesn't really matter here.\n",
    "        # But to be consistent with the gradient, I include it)\n",
    "        cost = np.sum(loss ** 2) / (2 * m)\n",
    "        if i < 100:\n",
    "            print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta\n",
    "\n",
    "\n",
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "m, n = np.shape(X_train)\n",
    "numIterations= 100000\n",
    "alpha = 0.1\n",
    "theta = np.ones(n)\n",
    "print theta\n",
    "theta = gradientDescent(X_train, Y_train, theta, alpha, m, numIterations)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (2+1+5+2 = 10 points)\n",
    "\n",
    "We will use Google's Tensorflow to create a simple multi-layered perceptron. Installation instructions can be found [here](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#pip-installation). To make our lives even easier, we will be using [skflow](https://github.com/tensorflow/skflow). This can be installed via pip install skflow. This is a higher level API on top of tensorflow. You can find documentation on how to get started on the skflow page.\n",
    "\n",
    "To install tensorflow, this command should work (did on Mac):\n",
    "\n",
    "sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.0-py2-none-any.whl --ignore-installed\n",
    "\n",
    "1. Use pandas to get spam classification [data](https://archive.ics.uci.edu/ml/datasets/Spambase) from UCI. Don't worry about getting the column names. The last column is a 1 if spam, zero otherwise.\n",
    "2. Split the data into training and testing using test_size=0.33, random_state=42.\n",
    "3. Use a TensorFlowDNNClassifier to classify whether an email is spam and report your testing accuracy. You should use 1 hidden layer with 5 units, 50,000 steps, and a learning rate of .05. What does each parameter do and why does it matter?\n",
    "4. Compare your accuracy to a logistic regression using sklean. Discuss why one may have performed better than the other. You may also experiment with the architecture of your neural network (i.e. the number of hidden units, the number of nodes, the number of steps, and the learning rate) to see if you can improve your results from part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\", sep = ',', \n",
    "                   header = None)\n",
    "spam = data[57]\n",
    "del data[57]\n",
    "#print(data.shape)\n",
    "#data[data[57] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "data_training, data_testing, spam_training, spam_testing = train_test_split(data, spam, \n",
    "                                                                            test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3    1 hidden layer, 5 units, 50000 steps, learning rate of .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1, avg. loss: 5.17732\n",
      "Step #5001, epoch #51, avg. loss: 0.80931\n",
      "Step #10001, epoch #103, avg. loss: 0.85908\n",
      "Step #15001, epoch #154, avg. loss: 0.77170\n",
      "Step #20001, epoch #206, avg. loss: 0.72194\n",
      "Step #25001, epoch #257, avg. loss: 0.69393\n",
      "Step #30001, epoch #309, avg. loss: 0.63862\n",
      "Step #35001, epoch #360, avg. loss: 0.62803\n",
      "Step #40001, epoch #412, avg. loss: 0.63735\n",
      "Step #45001, epoch #463, avg. loss: 0.62265\n",
      "Accuracy: 0.748519\n"
     ]
    }
   ],
   "source": [
    "import skflow\n",
    "from sklearn import datasets, metrics, linear_model\n",
    "\n",
    "classifier = skflow.TensorFlowDNNClassifier(hidden_units = [5], n_classes = 2, steps = 50000, learning_rate = 0.05)\n",
    "classifier.fit(data_training, spam_training)\n",
    "score = metrics.accuracy_score(classifier.predict(data_testing), spam_testing)\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n_classes represents how many classes we are trying to predict. In this question, they are 'spam' and 'not spam'. Steps are how many time this software is going to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.930876\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LogisticRegression()\n",
    "regr.fit(data_training, spam_training)\n",
    "regr_predict = regr.predict(data_testing)\n",
    "score = metrics.accuracy_score(regr_predict, spam_testing)\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The accuracy we got from logistic regression is way higher than the classifier, the reason of this could be not enough step of classifier.  If we let it runs for days, this accuracy could be much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (2+2+3 = 7 points)\n",
    "\n",
    "1. State briefly what you understand by the bias-variance tradeoff.\n",
    "\n",
    "2. For a given model and problem, what happens to these two quantities when the amount of training data available decreases, keeping all other factors remaining the same ( e.g. if 5-fold CV was used to train the original model, the same is used for the smaller dataset)?\n",
    "\n",
    "\n",
    "3. Suppose you want to approximate the pdf of a continuous random variable $X$, that takes on values over the interval (a,b), as follows: Get $N$ i.i.d samples of $X$; bin the interval into $k$ equi-spaced bins, and construct a histogram, which you then normalize so that total area under the histogram is 1. This normalized histogram will be an approximation of the true pdf. Clearly the histogram will change if you repeat this experiment using another $N$ samples; hence you can consider the quality of the solution in term of the 'mean' histogram (bias) and the variations among the histograms (variance).  Qualitatively explain how you would expect the bias-variance tradeoff to be reflected in this situation, as a function of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1\n",
    "It is the problem of stimultaneously minizing two sources of erro that prevent supervised learning from generalizing from their traing training sets.  Bias is the error from errorneous assumptions in the learing algorithm.  And variance is error from sensitivity to small fluctuation in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2\n",
    "If we reduce the sample size, the bias would decrease, but in the other hand, we also increased the source of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3\n",
    "Since $|\\hat{p}(x) - p(x)| = |\\hat{p}(x) - E[\\hat{p}(x)] + E[\\hat{p}(x)] - p[x]|$, which variance is $\\hat{p}(x) - E[\\hat{p}(x)]$ and bias is $E[\\hat{p}(x)] - p[x]$.  If bin-width is $\\delta$, the number of bins $k$ would be $1/\\delta$.  Then if the $\\delta$ is small and $k$ is large, it will result in small bias and large variance, if $\\delta$ is large and $k$ is small, it will result in large bias and small variance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
