{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-361M Introduction to Data Mining\n",
    "## Assignment #2\n",
    "## Due: Thursday, Feb 18, 2016 by 2pm; Total points: 50\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook** (if this isn't possible, let me know). Please use this naming format for your notebook you submit: **Group(Group Num)_HW(HW Number).ipynb**. For example, Group1_HW1.ipynb. Homeworks are due at the beginning of class on the due date and should be submitted through Canvas in your **groups of 3 from the first homework**. If groups need to be adjusted please contact the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1: Sampling\n",
    "### 10 points\n",
    "\n",
    "1. CBS has come up with an extreme TV show, and each of its viewers either likes or hates it. (no middle ground here; we are in a 'black and white age'). CBS wants to estimate what fraction $p$ of its audience like the show by 'randomly' calling $n$ viewers and tallying their responses so as to estimate the true value of $p$ to a fractional  accuracy of within $\\pm \\epsilon$%, with a confidence of $(1-\\alpha) \\times 100$%. For $\\alpha =  0.1$, $\\epsilon = 0.02$ (i.e. your answer will be $\\hat{p} \\pm 0.02$), what is the minimum value of $n$ needed if (i) true value $p = 0.5$ and (ii) $p = 0.95$? \n",
    "%(First try to do this yourself knowing that you have a binomial distribution, which can be approximated by a normal distribution. If you cannot, consult an undergrad stats book.)\n",
    "2. Suppose for a certain value  of $p$ and choice of $\\epsilon$, you calculate that you will need (at least) 1000 samples for $\\alpha = 0.1$. You now decide to obtain  a more accurate answer by either (i) reducing $\\alpha$ to 0.05, keeping the same $\\epsilon$ or by (ii) reducing $\\epsilon$ by a factor of 2 from the original value, but maintaining  $\\alpha = 0.1$.  In each case how many samples would you need now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Republican Presidental Debate\n",
    "### 10 points\n",
    "\n",
    "In this question we will be analyzing text data from one of the recent presidental debates. I have included code below to grab the data for you from the New York Times.\n",
    "\n",
    "1. Create a set of the frequency of utterance of  all the distinct words spoken by candidates, and then use it to create a histogram (with 30 bins) of word counts. Thus a bin is a range of count values and the corresponding \"y\" value is the number of words whose count falls in this range. What is interesting about this distribution? What are the 10 most common words?\n",
    "2. Remove the 100 most common words from vocabulary. Meaning that if you ever see this word, get rid of it. Now create a new python dictionary for each candidate that is a single list of all the words spoken by this candidate (ignoring these most common words). What are the 10 most common words for Trump, Rubio, and Cruz? How do their words differ?\n",
    "3. Using our dictionary from number 2, how many words did each speaker speak? Who spoke the most? Who is the outlier?\n",
    "4. Count the percentage of time each person uses the words (I, I'm, me, mine). When doing this convert all words to lower case. Create a bar plot of this percentage for each candidate with bars from largest to smallest. Use dictionary that has all words (doesn't exclude most common). What does the plot show?\n",
    "\n",
    "Hints:\n",
    "1. Look at python Counter.\n",
    "2. Just split text on a space. This isn't perfect, but will be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "url = 'http://www.nytimes.com/2015/11/11/us/politics/transcript-republican-presidential-debate.html'\n",
    "# requests gets the source code from the url and extracts it as text\n",
    "html = requests.get(url).text\n",
    "# beautifulsoup is a library that takes in text source code and returns a structured format of that\n",
    "# source code that you can more easily search and parse.\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "# get all the 'p' tags from the source with class = 'story-body-text'\n",
    "# this was determined by looking at the source code\n",
    "# the first and last paragraphs are intro and ending\n",
    "paragraphs = soup('p', {'class': 'story-body-text'})[1:-1]\n",
    "candidates = ['BUSH', 'TRUMP', 'RUBIO', \n",
    "              'CARSON', 'FIORINA', 'KASICH', 'CRUZ', 'PAUL']\n",
    "\n",
    "def text_to_dict(paragraph_array):\n",
    "    '''takes an array of text paragraphs from debate and returns dict \n",
    "    where key is person and value is list of text spoken by that candidate'''\n",
    "    # dict is like a hash map. defaultdict lets you specify what types of values will be in your hash map\n",
    "    d = defaultdict(list)\n",
    "    # just a default speaker that won't end up in our returned data\n",
    "    # will get replaced when an actual speaker is found\n",
    "    speaker = \"<START>\"\n",
    "    for paragraph in paragraph_array:\n",
    "        words = paragraph.text.split(' ')\n",
    "        first_word = words[0]\n",
    "        # only new speaker when have SPEAKER: format\n",
    "        if first_word[-1] == \":\":\n",
    "            speaker = first_word[:-1]\n",
    "        # only keep candidates text\n",
    "        if speaker in candidates:\n",
    "            d[speaker].append(words[1:])\n",
    "    return d\n",
    "\n",
    "speaker_dict = text_to_dict(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queston 3: Principal Component Analysis\n",
    "### 15 points\n",
    "\n",
    "In this question, you will explore an application of PCA.\n",
    "\n",
    "1. Convert your data from 3.2 to a vectorized format. This means you will have a row for each candidate and a column for each word in your data. A column for a candidate will contain the number of times that candidate used that word. Use [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html) from sklearn with min_df = 1.\n",
    "2. Convert your data from a sparse matrix to a dense array using .toarray() and then scale it to have mean zero and standard deviation of 1. See [here](http://scikit-learn.org/stable/modules/preprocessing.html) for help.\n",
    "2. Plot the explained variance as a function of the number of PCA components (called a scree plot). Use sklearn's PCA functionality to do this.\n",
    "3. Now pick the top two principal components and project the data onto the respective dimensions. Visualize the data in a scatter plot and label each point with the candidate's name. Who are the outliers? Use sklearn and matplotlib for this. \n",
    "4.  In what sense is PCA an optimal feature extraction technique? Describe a situation where you would prefer feature selection to (linear) feature extraction, even though the former  is a special case of the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Robust Regression\n",
    "### 5 points\n",
    "\n",
    "In this question we will be exploring using a regression technique that is more robust to outliers. I provide some code below that injects outlier points into the original medv and lstat data from the housing dataset. This problem looks into how robust regression can help in the presence of outliers.\n",
    "\n",
    "1. Using the original data, plot lstat on the x-axis and log(medv) on the y-axis of a scatter plot with the line of best fit from a linear regression on the plot as well. Do the same, but with the data that includes the outlier values. What changes with the best fit line? Specifically, how does the slope change?\n",
    "2. Now run a linear regression with a Huber loss on the data including the outliers and create the same plot as above, but this time with the fit from the Huber loss regression (using all the data). What has changed (comment on the slope as well)? Note: Use SGDRegressor from sklearn with 500 iterations and no penalty.\n",
    "3. Explain why the huber loss is more robust to outliers.\n",
    "\n",
    "Note:  Use plot's with xlim = (-5, 40) and ylim = (1, 5). These set the range for the x any y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "housing_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n",
    "                   delim_whitespace=True, header=None,\n",
    "                   names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO',\n",
    "                           'B', 'LSTAT', 'MEDV'])\n",
    "housing_data = housing_data.dropna()\n",
    "lstat = housing_data.LSTAT.values\n",
    "medv = housing_data.MEDV.values\n",
    "medv_std = np.std(medv)\n",
    "lstat_std = np.std(lstat)\n",
    "np.random.seed(42)\n",
    "medv_outliers = np.random.normal(1, 1, 5)\n",
    "lstat_outliers = np.random.normal(1, 1, 5)\n",
    "medv_with_outliers = np.append(medv, medv_outliers)\n",
    "lstat_with_outliers = np.append(lstat, lstat_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Visualization using Bokeh\n",
    "## 10 points\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the [auto-mpg](http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original) data, your goal is to build a Bokeh visualization which allows the user explore how MPG varies with horsepower and weight. You will create a visualization that allows the user to toggle the Y axis of a scatter plot between horsepower and weight. With the x-axis always being MPG.\n",
    "\n",
    "Hints: \n",
    "1. You can make use of Select widgets.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/user_guide/interaction.html#javascript-callbacks. Specifically look at the CustomJS for Widgets under Callbacks and the Select widget. \n",
    "3. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "4. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "5. Use output_notebook() from Bokeh to output the plot to your notebook\n",
    "\n",
    "We have made available a sample screenshot of our Bokeh app that supports the above requirements. Your interface should look similar to the screenshots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
